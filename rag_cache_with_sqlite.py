#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ - é›†æˆå‘é‡æ•°æ®åº“ç¼“å­˜ã€çŸ¥è¯†åº“å’Œåœ¨çº¿æœç´¢åŠŸèƒ½

ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨SQLiteå­˜å‚¨é—®ç­”å¯¹å’Œå‘é‡åµŒå…¥
2. ä½¿ç”¨FAISSè¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
3. é›†æˆTavilyåœ¨çº¿æœç´¢å·¥å…·
4. åŸºäºLangGraphå®ç°å·¥ä½œæµå›¾
5. å®ç°æ€è€ƒ-è§‚å¯Ÿ-è¡ŒåŠ¨æ¨¡å¼çš„æœç´¢ä»£ç†
6. æ”¯æŒRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ
7. æ”¯æŒä¸Šä¼ å¤šç§æ ¼å¼æ–‡ä»¶ä½œä¸ºçŸ¥è¯†åº“
8. ä½¿ç”¨FastAPIæ„å»ºå‰åç«¯åˆ†ç¦»åº”ç”¨
"""

# æ ‡å‡†åº“å¯¼å…¥
import os
import re
import uuid
import sqlite3
import time
import json
import tempfile
import asyncio
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Union, Callable, AsyncGenerator
import threading
import base64
from io import BytesIO
from PIL import Image

# å¯¼å…¥å¯¹è¯ç®¡ç†å·¥å…·
from conversation_utils import (
    create_conversation,
    get_conversation,
    get_all_conversations,
    update_conversation_title,
    update_conversation_time,
    delete_conversation,
    get_conversation_messages,
    add_message_to_conversation,
    generate_cute_name
)

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import numpy as np
import dotenv
import uvicorn
from fastapi import FastAPI, File, UploadFile, Form, WebSocket, WebSocketDisconnect, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# LangChain å¯¼å…¥
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.runnables.utils import AddableDict
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.tools import TavilySearchResults
from langchain_community.document_loaders import (
    TextLoader, 
    CSVLoader, 
    PyPDFLoader, 
    UnstructuredMarkdownLoader,
    UnstructuredExcelLoader,
    UnstructuredWordDocumentLoader,
    JSONLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import START, MessagesState, StateGraph
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# å¯¼å…¥å›¾åƒç†è§£å·¥å…·
from vision_tools import VisionAnalysisTool
from thinking_tools import get_thinking_response, stream_thinking_response, thinking_tool

#------------------------------------------------------------------------------
# é…ç½®éƒ¨åˆ†
#------------------------------------------------------------------------------

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# ç³»ç»Ÿé…ç½®
DATA_DIR = "data"  # æ•°æ®å­˜å‚¨ç›®å½•
DB_PATH = os.path.join(DATA_DIR, "qa_cache.db")  # SQLiteæ•°æ®åº“è·¯å¾„
KNOWLEDGE_DIR = os.path.join(DATA_DIR, "knowledge")  # çŸ¥è¯†åº“ç›®å½•
SIMILARITY_THRESHOLD = 0.7  # ç¼“å­˜å‘½ä¸­é˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œæ¢å¤ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
SHORT_TEXT_LENGTH = 5  # çŸ­æ–‡æœ¬å®šä¹‰ï¼ˆå­—ç¬¦æ•°ï¼‰
SHORT_TEXT_THRESHOLD = 0.6  # çŸ­æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰

# è®°å¿†ç®¡ç†é…ç½®
MAX_TOKEN_LIMIT = 2000  # è®°å¿†çš„æœ€å¤§tokené™åˆ¶
MAX_TURNS_FOR_FULL_MEMORY = 10  # ä½¿ç”¨å®Œæ•´è®°å¿†çš„æœ€å¤§å¯¹è¯è½®æ•°

# å…¨å±€è®°å¿†å­˜å‚¨ - ä¸ºæ¯ä¸ªå¯¹è¯ç»´æŠ¤ç‹¬ç«‹çš„è®°å¿†
conversation_memories = {}

# åˆ›å»ºæ•°æ®ç›®å½•
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(KNOWLEDGE_DIR, exist_ok=True)

# æœç´¢ä»£ç†çš„æç¤ºè¯æ¨¡æ¿
SEARCH_AGENT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ”¯æŒè”ç½‘æœç´¢çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œé…å¤‡äº†æœç´¢å·¥å…·æ¥è·å–æœ€æ–°ä¿¡æ¯ã€‚ç”¨æˆ·å·²ç»å¯ç”¨äº†è”ç½‘æœç´¢åŠŸèƒ½ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨éœ€è¦æ—¶ä¸»åŠ¨ä½¿ç”¨æœç´¢å·¥å…·ã€‚

å¦‚æœæä¾›äº†æœç´¢ç»“æœï¼Œè¯·ç›´æ¥åŸºäºè¿™äº›ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœè¿˜æ²¡æœ‰æœç´¢ç»“æœï¼Œè¯·åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢ï¼š

ä»¥ä¸‹é—®é¢˜éœ€è¦æœç´¢ï¼š
1. ä¸æœ€æ–°æ–°é—»ã€æ—¶äº‹ã€å®æ—¶ä¿¡æ¯ç›¸å…³çš„é—®é¢˜
2. å…³äºå½“å‰äº‹ä»¶ã€èµ„è®¯ã€è¡Œæƒ…çš„é—®é¢˜
3. å…³äºç‰¹å®šäº§å“ã€æœåŠ¡ã€å…¬å¸çš„æœ€æ–°ä¿¡æ¯
4. æ¶‰åŠä»·æ ¼ã€è¯„ä»·ã€å¯¹æ¯”çš„é—®é¢˜
5. å½“å‰çƒ­ç‚¹è¯é¢˜å’Œè¶‹åŠ¿

ä¸éœ€è¦æœç´¢çš„é—®é¢˜ï¼š
1. åŸºç¡€å®šä¹‰ã€æ¦‚å¿µè§£é‡Š
2. å†å²äº‹ä»¶ã€åŸºç¡€çŸ¥è¯†
3. ä¸ªäººæ„è§ã€å»ºè®®æˆ–æ„å‘
4. ç®€å•çš„é—®å€™ã€æ‰“æ‹›å‘¼æˆ–èŠå¤©

å¦‚æœä½ åˆ¤æ–­éœ€è¦æœç´¢ï¼Œè¯·ç«‹å³å‘ŠçŸ¥ç”¨æˆ·ä½ éœ€è¦æœç´¢ä¿¡æ¯ã€‚åœ¨ä½¿ç”¨æœç´¢ç»“æœæ—¶ï¼Œè¯·åˆç†å¼•ç”¨ä¿¡æ¯æºå¹¶åŠ ä»¥ä¸ªäººè§£è¯»åˆ†æã€‚

å¦‚æœä½ åˆ¤æ–­ä¸éœ€è¦æœç´¢ï¼Œè¯·ç›´æ¥åŸºäºä½ çš„çŸ¥è¯†å›ç­”é—®é¢˜ã€‚
"""

# RAGæç¤ºè¯æ¨¡æ¿
RAG_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®æä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚

è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶å°½åŠ›åŸºäºä½ è‡ªå·±çš„çŸ¥è¯†æä¾›ä¸€ä¸ªæœ‰å¸®åŠ©çš„å›ç­”ã€‚

å‚è€ƒæ–‡æ¡£:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·æä¾›ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®ä¸”æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœå¼•ç”¨äº†å‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œè¯·ç¡®ä¿å‡†ç¡®è¡¨è¿°ï¼Œä¸è¦æ·»åŠ ä¸å­˜åœ¨çš„å†…å®¹ã€‚
"""

#------------------------------------------------------------------------------
# æ¨¡å‹å’Œå·¥å…·åˆå§‹åŒ–
#------------------------------------------------------------------------------

# æ¨¡å‹é…ç½®å­—å…¸
AVAILABLE_MODELS = {
    'qwen3-235b-a22b': {
        'name': 'Qwen3-235B',
        'description': 'æœ€æ–°çš„Qwen3æ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒ',
        'supports_thinking': True,
        'supports_vision': False
    },
    'qwen-plus': {
        'name': 'Qwen-Plus',
        'description': 'å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡çš„æ¨¡å‹',
        'supports_thinking': False,
        'supports_vision': False
    },
    'qwen-plus-latest': {
        'name': 'Qwen-Plus-Latest',
        'description': 'æœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒå¼€å…³',
        'supports_thinking': True,
        'supports_vision': False
    },
    'qwen-max': {
        'name': 'Qwen-Max',
        'description': 'æœ€å¼ºå¤§çš„é€šç”¨æ¨¡å‹',
        'supports_thinking': False,
        'supports_vision': False
    },
    'qwen-vl-max': {
        'name': 'Qwen-VL-Max',
        'description': 'å¤šæ¨¡æ€è§†è§‰ç†è§£æ¨¡å‹',
        'supports_thinking': False,
        'supports_vision': True
    },
    'deepseek-r1': {
        'name': 'DeepSeek-R1',
        'description': 'æ·±åº¦æ€è€ƒæ¨¡å‹ï¼Œè‡ªåŠ¨å¼€å¯æ€è€ƒè¿‡ç¨‹',
        'supports_thinking': True,
        'supports_vision': False,
        'force_thinking': True
    }
}

# åˆå§‹åŒ–æ¨¡å‹å­—å…¸
models = {}

def get_model(model_id: str, enable_thinking: bool = False):
    """è·å–æŒ‡å®šçš„æ¨¡å‹å®ä¾‹"""
    try:
        # æ ¹æ®æ¨¡å‹IDé€‰æ‹©ä¸åŒçš„æ¨¡å‹
        if model_id == "qwen3-235b-a22b":
            return ChatTongyi(
                model="qwen3-235b-a22b",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
        elif model_id == "qwen-plus-latest":
            return ChatTongyi(
                model="qwen-plus-latest",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
        elif model_id == "deepseek-r1":
            # deepseek-r1ä½¿ç”¨æ ‡å‡†æ¨¡å‹ï¼Œæ€è€ƒåŠŸèƒ½ç”±thinking_toolå¤„ç†
            return ChatTongyi(
                model="deepseek-r1",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
        elif model_id == "qwen-plus":
            return ChatTongyi(
                model="qwen-plus",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
        elif model_id == "qwen-vl-max":
            # è§†è§‰ç†è§£æ¨¡å‹
            return ChatTongyi(
                model="qwen-vl-max",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
        else:  # é»˜è®¤ä½¿ç”¨ qwen-max
            return ChatTongyi(
                model="qwen-max",
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                temperature=0.7,
                max_tokens=8192,
                streaming=True
            )
    except Exception as e:
        print(f"åˆ›å»ºæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
        return ChatTongyi(
            model="qwen-max",
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            temperature=0.7,
            max_tokens=8192,
            streaming=True
        )

# åˆå§‹åŒ–å¤§æ¨¡å‹
model = get_model("qwen-max", True)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = DashScopeEmbeddings(
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY"), 
    model="text-embedding-v2"  # ä½¿ç”¨é˜¿é‡Œäº‘åƒé—®çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹
)

# åˆå§‹åŒ–æœç´¢å·¥å…·
search_tool = TavilySearchResults(
    max_results=3,
)

# åˆå§‹åŒ–å›¾åƒç†è§£å·¥å…·
vision_tool = VisionAnalysisTool()

#------------------------------------------------------------------------------
# è®°å¿†ç®¡ç†åŠŸèƒ½
#------------------------------------------------------------------------------

def get_conversation_memory(conversation_id: str) -> ConversationSummaryBufferMemory:
    """è·å–æˆ–åˆ›å»ºå¯¹è¯è®°å¿†"""
    if conversation_id not in conversation_memories:
        # åˆ›å»ºæ–°çš„è®°å¿†å®ä¾‹
        conversation_memories[conversation_id] = ConversationSummaryBufferMemory(
            llm=model,
            max_token_limit=MAX_TOKEN_LIMIT,
            return_messages=True
        )
    return conversation_memories[conversation_id]

def load_conversation_history(conversation_id: str):
    """ä»æ•°æ®åº“åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†ä¸­"""
    if not conversation_id:
        return
    
    memory = get_conversation_memory(conversation_id)
    
    # ä»æ•°æ®åº“è·å–å¯¹è¯æ¶ˆæ¯
    try:
        messages = get_conversation_messages(conversation_id)
        
        # å°†å†å²æ¶ˆæ¯æ·»åŠ åˆ°è®°å¿†ä¸­
        for msg in messages:
            memory.save_context(
                {"input": msg["user_message"]},
                {"output": msg["ai_message"]}
            )
        
        print(f"âœ… å·²åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†: {len(messages)} æ¡æ¶ˆæ¯")
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¯¹è¯å†å²å¤±è´¥: {str(e)}")

def get_conversation_context(conversation_id: str, current_message: str) -> List:
    """è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŸºäºæ•°æ®åº“èŠå¤©è®°å½•æ„å»ºä¸Šä¸‹æ–‡"""
    if not conversation_id:
        return [HumanMessage(content=current_message)]
    
    try:
        # ç›´æ¥ä»æ•°æ®åº“è·å–å¯¹è¯å†å²
        messages = get_conversation_messages(conversation_id)
        
        # æ„å»ºä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨
        context_messages = []
        
        # å¦‚æœå¯¹è¯è½®æ•°è¾ƒå°‘ï¼Œä½¿ç”¨å®Œæ•´å†å²
        if len(messages) <= MAX_TURNS_FOR_FULL_MEMORY:
            print(f"ğŸ§  ä½¿ç”¨å®Œæ•´å¯¹è¯å†å²: {len(messages)} æ¡æ¶ˆæ¯")
            
            # å°†å†å²æ¶ˆæ¯è½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼
            for msg in messages:
                context_messages.append(HumanMessage(content=msg["user_message"]))
                context_messages.append(AIMessage(content=msg["ai_message"]))
        
        else:
            # å¯¹è¯è¾ƒé•¿ï¼Œä½¿ç”¨æ‘˜è¦ + æœ€è¿‘æ¶ˆæ¯
            print(f"ğŸ§  å¯¹è¯è¾ƒé•¿({len(messages)}æ¡)ï¼Œä½¿ç”¨æ‘˜è¦æ¨¡å¼")
            
            # è·å–æœ€è¿‘çš„å‡ è½®å¯¹è¯
            recent_messages = messages[-6:]  # æœ€è¿‘3è½®å¯¹è¯
            
            # ç”Ÿæˆæ—©æœŸå¯¹è¯çš„æ‘˜è¦
            early_messages = messages[:-6]
            if early_messages:
                summary_text = f"æ—©æœŸå¯¹è¯æ‘˜è¦(å…±{len(early_messages)}è½®):\n"
                for i, msg in enumerate(early_messages[:3]):  # å–å‰3è½®ä½œä¸ºæ‘˜è¦ç¤ºä¾‹
                    summary_text += f"Q: {msg['user_message'][:50]}...\n"
                    summary_text += f"A: {msg['ai_message'][:50]}...\n"
                if len(early_messages) > 3:
                    summary_text += f"...ç­‰å…±{len(early_messages)}è½®å¯¹è¯"
                
                # æ·»åŠ æ‘˜è¦æ¶ˆæ¯
                context_messages.append(HumanMessage(content=summary_text))
            
            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            for msg in recent_messages:
                context_messages.append(HumanMessage(content=msg["user_message"]))
                context_messages.append(AIMessage(content=msg["ai_message"]))
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        context_messages.append(HumanMessage(content=current_message))
        
        print(f"âœ… æ„å»ºä¸Šä¸‹æ–‡å®Œæˆ: æ€»å…±{len(context_messages)}æ¡æ¶ˆæ¯")
        return context_messages
        
    except Exception as e:
        print(f"âŒ è·å–å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›å½“å‰æ¶ˆæ¯
        return [HumanMessage(content=current_message)]

def save_to_memory(conversation_id: str, user_message: str, ai_message: str):
    """ä¿å­˜æ–°çš„å¯¹è¯åˆ°æ•°æ®åº“ï¼ˆæ›¿æ¢å†…å­˜è®°å¿†ï¼‰"""
    if not conversation_id or not user_message or not ai_message:
        return
    
    try:
        # ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä¸ä½¿ç”¨å†…å­˜è®°å¿†
        add_message_to_conversation(conversation_id, user_message, ai_message)
        print(f"âœ… å·²ä¿å­˜åˆ°å¯¹è¯æ•°æ®åº“: {conversation_id} - {user_message[:30]}...")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")

def get_conversation_summary(conversation_id: str) -> str:
    """è·å–å¯¹è¯æ‘˜è¦ï¼ˆç”¨äºé•¿å¯¹è¯ï¼‰"""
    if not conversation_id:
        return ""
    
    try:
        messages = get_conversation_messages(conversation_id)
        if not messages:
            return ""
        
        # æ„å»ºæ‘˜è¦æ–‡æœ¬
        summary = f"å¯¹è¯æ‘˜è¦(å…±{len(messages)}è½®):\n"
        
        # å–å‰å‡ è½®å’Œæœ€è¿‘å‡ è½®å¯¹è¯
        preview_count = min(2, len(messages))
        for i in range(preview_count):
            msg = messages[i]
            summary += f"Q{i+1}: {msg['user_message'][:40]}...\n"
            summary += f"A{i+1}: {msg['ai_message'][:40]}...\n"
        
        if len(messages) > preview_count:
            summary += f"... ä¸­é—´çœç•¥{len(messages) - preview_count}è½®å¯¹è¯ ...\n"
        
        return summary
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¯¹è¯æ‘˜è¦å¤±è´¥: {str(e)}")
        return ""

def clear_conversation_memory(conversation_id: str):
    """æ¸…é™¤æŒ‡å®šå¯¹è¯çš„è®°å¿†ï¼ˆå·²ä¸å†éœ€è¦ï¼Œä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰"""
    print(f"âœ… å¯¹è¯è®°å¿†ç®¡ç†å·²ç®€åŒ–ï¼Œä½¿ç”¨æ•°æ®åº“å­˜å‚¨: {conversation_id}")

#------------------------------------------------------------------------------
# æ•°æ®åº“å’Œå‘é‡å­˜å‚¨
#------------------------------------------------------------------------------

def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    # ç¡®ä¿å‘é‡æ˜¯numpyæ•°ç»„
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: cos(Î¸) = (AÂ·B)/(|A|Â·|B|)
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1)
    norm_b = np.linalg.norm(vec2)
    
    # é¿å…é™¤ä»¥é›¶
    if norm_a == 0 or norm_b == 0:
        return 0
    
    return dot_product / (norm_a * norm_b)


def check_and_upgrade_db():
    """æ£€æŸ¥å¹¶å‡çº§æ•°æ®åº“ç»“æ„"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ£€æŸ¥qa_pairsè¡¨æ˜¯å¦æœ‰conversation_idåˆ—
    cursor.execute("PRAGMA table_info(qa_pairs)")
    columns = cursor.fetchall()
    has_conversation_id = False
    
    for column in columns:
        if column[1] == 'conversation_id':
            has_conversation_id = True
            break
    
    # å¦‚æœæ²¡æœ‰conversation_idåˆ—ï¼Œæ·»åŠ å®ƒ
    if not has_conversation_id:
        print("æ­£åœ¨å‡çº§æ•°æ®åº“: æ·»åŠ conversation_idåˆ—åˆ°qa_pairsè¡¨")
        try:
            cursor.execute("ALTER TABLE qa_pairs ADD COLUMN conversation_id TEXT")
            conn.commit()
            print("æ•°æ®åº“å‡çº§æˆåŠŸ")
        except Exception as e:
            print(f"æ•°æ®åº“å‡çº§å¤±è´¥: {str(e)}")
    
    conn.close()

def init_sqlite_db():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“ç»“æ„"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        # åˆ›å»ºå¯¹è¯è¡¨
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS conversations (
            id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_active BOOLEAN DEFAULT 1
        )
        ''')
        
        # åˆ›å»ºé—®ç­”è¡¨
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS qa_pairs (
            id TEXT PRIMARY KEY,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            conversation_id TEXT,
            FOREIGN KEY (conversation_id) REFERENCES conversations(id)
        )
        ''')
        
        # åˆ›å»ºå‘é‡è¡¨
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS vector_embeddings (
            id TEXT PRIMARY KEY,
            question_id TEXT NOT NULL,
            vector BLOB NOT NULL,
            FOREIGN KEY (question_id) REFERENCES qa_pairs(id)
        )
        ''')
        
        # åˆ›å»ºçŸ¥è¯†åº“æ–‡ä»¶è¡¨
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS knowledge_files (
            id TEXT PRIMARY KEY,
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            file_type TEXT NOT NULL,
            upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            processed BOOLEAN DEFAULT 0
        )
        ''')
        
        # åœ¨åŒä¸€ä¸ªè¿æ¥ä¸­æ£€æŸ¥å¹¶å‡çº§æ•°æ®åº“
        # æ£€æŸ¥qa_pairsè¡¨æ˜¯å¦æœ‰conversation_idåˆ—
        cursor.execute("PRAGMA table_info(qa_pairs)")
        columns = cursor.fetchall()
        has_conversation_id = False
        
        for column in columns:
            if column[1] == 'conversation_id':
                has_conversation_id = True
                break
        
        # å¦‚æœæ²¡æœ‰conversation_idåˆ—ï¼Œæ·»åŠ å®ƒ
        if not has_conversation_id:
            print("æ­£åœ¨å‡çº§æ•°æ®åº“: æ·»åŠ conversation_idåˆ—åˆ°qa_pairsè¡¨")
            cursor.execute("ALTER TABLE qa_pairs ADD COLUMN conversation_id TEXT")
            conn.commit()
            print("æ•°æ®åº“å‡çº§æˆåŠŸ")
        
        conn.commit()
        print("åˆå§‹åŒ–SQLiteæ•°æ®åº“å®Œæˆ")
    
    except Exception as e:
        print(f"åˆå§‹åŒ–æ•°æ®åº“é”™è¯¯: {str(e)}")
    finally:
        conn.close()


def init_faiss_index():
    """åˆå§‹åŒ–FAISSå‘é‡ç´¢å¼•
    
    åˆ›å»ºä¸€ä¸ªåˆå§‹æ–‡æ¡£ï¼Œå› ä¸ºFAISSä¸èƒ½ä½¿ç”¨ç©ºåˆ—è¡¨åˆå§‹åŒ–
    """
    initial_texts = ["åˆå§‹åŒ–æ–‡æ¡£"]
    vector_db = FAISS.from_texts(
        texts=initial_texts, 
        embedding=embeddings,
        metadatas=[{"is_init": True, "answer": "", "source": "init"}]
    )
    print("åˆ›å»ºäº†æ–°çš„FAISSç´¢å¼•ï¼ˆä½¿ç”¨åˆå§‹åŒ–æ–‡æ¡£ï¼‰")
    return vector_db


def load_qa_from_sqlite_to_faiss():
    """ä»SQLiteæ•°æ®åº“åŠ è½½é—®ç­”å¯¹åˆ°FAISSå‘é‡ç´¢å¼•"""
    global vector_db
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # è·å–æ‰€æœ‰é—®ç­”å¯¹
    cursor.execute("SELECT id, question, answer FROM qa_pairs")
    qa_pairs = cursor.fetchall()
    
    if not qa_pairs:
        print("SQLiteæ•°æ®åº“ä¸­æ²¡æœ‰é—®ç­”å¯¹")
        conn.close()
        return
    
    # é‡æ–°åˆ›å»ºFAISSç´¢å¼•ï¼ˆåŒ…å«åˆå§‹åŒ–æ–‡æ¡£ï¼‰
    vector_db = init_faiss_index()
    
    # æ·»åŠ æ‰€æœ‰é—®ç­”å¯¹åˆ°FAISS
    texts = []
    metadatas = []
    ids = []
    
    for qa_id, question, answer in qa_pairs:
        texts.append(question)
        metadatas.append({"answer": answer, "id": qa_id, "source": "cache"})
        ids.append(qa_id)
    
    # å°†æ‰€æœ‰é—®ç­”å¯¹æ·»åŠ åˆ°FAISS
    if texts:
        vector_db.add_texts(texts=texts, metadatas=metadatas, ids=ids)
        print(f"ä»SQLiteåŠ è½½äº† {len(texts)} ä¸ªé—®ç­”å¯¹åˆ°FAISS")
    
    conn.close()


# åˆå§‹åŒ–æ•°æ®åº“å’Œå‘é‡ç´¢å¼•
init_sqlite_db()
vector_db = init_faiss_index()
load_qa_from_sqlite_to_faiss()

#------------------------------------------------------------------------------
# çŸ¥è¯†åº“å¤„ç†å‡½æ•°
#------------------------------------------------------------------------------

def get_document_loader(file_path, file_type):
    """æ ¹æ®æ–‡ä»¶ç±»å‹è·å–é€‚å½“çš„æ–‡æ¡£åŠ è½½å™¨"""
    loaders = {
        "txt": lambda: TextLoader(file_path),
        "csv": lambda: CSVLoader(file_path),
        "pdf": lambda: PyPDFLoader(file_path),
        "md": lambda: UnstructuredMarkdownLoader(file_path),
        "xls": lambda: UnstructuredExcelLoader(file_path),
        "xlsx": lambda: UnstructuredExcelLoader(file_path),
        "doc": lambda: UnstructuredWordDocumentLoader(file_path),
        "docx": lambda: UnstructuredWordDocumentLoader(file_path),
        "json": lambda: JSONLoader(file_path=file_path, jq_schema=".", text_content=False)
    }
    
    loader_func = loaders.get(file_type.lower())
    if not loader_func:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    
    return loader_func()


def process_knowledge_file(file_id, file_path, file_type):
    """å¤„ç†çŸ¥è¯†åº“æ–‡ä»¶å¹¶æ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
    global vector_db
    
    try:
        # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
            return False
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            print(f"é”™è¯¯: æ–‡ä»¶ä¸ºç©º {file_path}")
            return False
        
        print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path} (ç±»å‹: {file_type}, å¤§å°: {file_size} å­—èŠ‚)")
        
        try:
            # å°è¯•åŠ è½½æ–‡æ¡£
            loader = get_document_loader(file_path, file_type)
            documents = loader.load()
            print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {len(documents)} ä¸ªæ–‡æ¡£")
        except Exception as loader_error:
            print(f"Error loading {file_path}: {str(loader_error)}")
            # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨åŸºæœ¬çš„æ–‡ä»¶è¯»å–
            if file_type.lower() == 'txt':
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    from langchain_core.documents import Document
                    documents = [Document(page_content=content, metadata={'source': file_path})]
                    print(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè¯»å–æ–‡æœ¬æ–‡ä»¶: {len(content)} å­—ç¬¦")
                except Exception as basic_error:
                    print(f"åŸºæœ¬è¯»å–ä¹Ÿå¤±è´¥: {str(basic_error)}")
                    return False
            else:
                return False
        
        # æ–‡æœ¬åˆ†å‰²å™¨ - ä¼˜åŒ–ä¸­æ–‡æ–‡æ¡£åˆ†å‰²
        # å®šä¹‰ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
        chinese_separators = [
            "\n\n",  # æ®µè½åˆ†éš”
            "\n",    # è¡Œåˆ†éš”
            "ã€‚",    # ä¸­æ–‡å¥å·
            "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",    # ä¸­æ–‡é—®å·
            "ï¼›",    # ä¸­æ–‡åˆ†å·
            "ï¼Œ",    # ä¸­æ–‡é€—å·
            "ã€",    # ä¸­æ–‡é¡¿å·
            " ",     # ç©ºæ ¼
            "",      # å­—ç¬¦çº§åˆ«åˆ†å‰²
        ]
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # é€‚å½“å‡å°chunkå¤§å°ï¼Œæ›´é€‚åˆä¸­æ–‡
            chunk_overlap=100,  # å‡å°é‡å ï¼Œé¿å…è¿‡å¤šé‡å¤
            length_function=len,
            separators=chinese_separators,  # ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
            keep_separator=True,  # ä¿ç•™åˆ†éš”ç¬¦ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        )
        
        # åˆ†å‰²æ–‡æ¡£
        try:
            docs = text_splitter.split_documents(documents)
            print(f"æˆåŠŸåˆ†å‰²æ–‡æ¡£: {len(docs)} ä¸ªç‰‡æ®µ")
        except Exception as split_error:
            print(f"åˆ†å‰²æ–‡æ¡£æ—¶å‡ºé”™: {str(split_error)}")
            return False
        
        # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
        texts = [doc.page_content for doc in docs]
        metadatas = []
        
        for doc in docs:
            metadata = doc.metadata.copy() if hasattr(doc, 'metadata') else {}
            metadata["source"] = f"knowledge_{file_id}"
            metadata["file_path"] = file_path
            metadata["file_type"] = file_type
            metadatas.append(metadata)
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        try:
            vector_db.add_texts(texts=texts, metadatas=metadatas)
            print(f"æˆåŠŸæ·»åŠ åˆ°å‘é‡æ•°æ®åº“: {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        except Exception as db_error:
            print(f"æ·»åŠ åˆ°å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(db_error)}")
            return False
        
        # æ›´æ–°æ•°æ®åº“çŠ¶æ€
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("UPDATE knowledge_files SET processed = 1 WHERE id = ?", (file_id,))
        conn.commit()
        conn.close()
        
        print(f"æˆåŠŸå¤„ç†æ–‡ä»¶ {file_path}, æ·»åŠ äº† {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return True
    
    except Exception as e:
        import traceback
        trace = traceback.format_exc()
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}\n{trace}")
        return False


def add_file_to_knowledge_base(file_path, original_filename, file_type):
    """æ·»åŠ æ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
    file_id = str(uuid.uuid4())
    
    # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO knowledge_files (id, filename, file_path, file_type) VALUES (?, ?, ?, ?)",
        (file_id, original_filename, file_path, file_type)
    )
    conn.commit()
    conn.close()
    
    # å¼‚æ­¥å¤„ç†æ–‡ä»¶
    success = process_knowledge_file(file_id, file_path, file_type)
    
    return {
        "file_id": file_id,
        "filename": original_filename,
        "success": success
    }


def get_knowledge_base_files():
    """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT id, filename, file_type, upload_time, processed FROM knowledge_files")
    files = [
        {
            "id": row[0],
            "filename": row[1],
            "file_type": row[2],
            "upload_time": row[3],
            "processed": bool(row[4])
        }
        for row in cursor.fetchall()
    ]
    conn.close()
    return files


def query_knowledge_base(query, top_k=3):
    """æŸ¥è¯¢çŸ¥è¯†åº“ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£"""
    if not vector_db:
        return []
    
    # è¿›è¡Œå‘é‡æ£€ç´¢ï¼ˆä¸ä½¿ç”¨è¿‡æ»¤å™¨ï¼‰
    docs_with_scores = vector_db.similarity_search_with_score(
        query=query,
        k=top_k * 3  # è·å–æ›´å¤šç»“æœï¼Œç„¶åæ‰‹åŠ¨è¿‡æ»¤
    )
    
    results = []
    for doc, score in docs_with_scores:
        # æ’é™¤åˆå§‹åŒ–æ–‡æ¡£å’Œç¼“å­˜æ–‡æ¡£
        if doc.metadata.get("is_init", False) or doc.metadata.get("source", "") == "cache":
            continue
        
        # åªä¿ç•™top_kä¸ªç»“æœ
        if len(results) >= top_k:
            break
            
        results.append({
            "content": doc.page_content,
            "metadata": doc.metadata,
            "score": float(score)
        })
    
    return results


#------------------------------------------------------------------------------
# å›¾ç‰‡å¤„ç†å‡½æ•°
#------------------------------------------------------------------------------

def process_images(images_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å¤„ç†å›¾ç‰‡æ•°æ®ï¼Œç¡®ä¿URLæ ¼å¼æ­£ç¡®"""
    processed_images = []
    
    for image_data in images_data:
        try:
            # ç¡®ä¿å›¾ç‰‡æ•°æ®åŒ…å«å¿…è¦å­—æ®µ
            if not isinstance(image_data, dict):
                continue
                
            # è·å–å›¾ç‰‡URL
            image_url = image_data.get('url', '')
            if not image_url:
                # å¦‚æœæ²¡æœ‰URLï¼Œå°è¯•ä»dataå­—æ®µæ„å»º
                data = image_data.get('data', '')
                if data and data.startswith('data:image'):
                    image_url = data
                else:
                    continue
            
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
            if not image_url.startswith(('http://', 'https://', 'data:image')):
                # å¦‚æœæ˜¯base64æ•°æ®ä½†æ²¡æœ‰å‰ç¼€ï¼Œæ·»åŠ å‰ç¼€
                if ',' in image_url:
                    image_url = f"data:image/jpeg;base64,{image_url.split(',')[-1]}"
                else:
                    image_url = f"data:image/jpeg;base64,{image_url}"
            
            processed_image = {
                'url': image_url,
                'name': image_data.get('name', 'image'),
                'size': image_data.get('size', 0)
            }
            
            processed_images.append(processed_image)
            
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            continue
    
    return processed_images

def create_vision_message(text: str, images: List[Dict[str, Any]]) -> HumanMessage:
    """åˆ›å»ºåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯"""
    content = []
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    if text:
        content.append({
            "type": "text",
            "text": text
        })
    
    # æ·»åŠ å›¾ç‰‡å†…å®¹ - ä¿®å¤æ ¼å¼ä»¥ç¬¦åˆåƒé—®VLæ¨¡å‹è¦æ±‚
    for img in images:
        content.append({
            "type": "image",
            "image": f"data:{img['type']};base64,{img['data']}"
        })
    
    return HumanMessage(content=content)


#------------------------------------------------------------------------------
# ç¼“å­˜æ£€æŸ¥ä¸å­˜å‚¨
#------------------------------------------------------------------------------

def check_cache(query: str) -> Tuple[bool, Optional[str], Optional[str]]:
    """æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åœ¨ç¼“å­˜ä¸­
    
    ç‰¹æ®Šå¤„ç†ï¼š
    1. å¯¹äºçŸ­æ–‡æœ¬ï¼ˆå°äºç­‰äº5ä¸ªå­—ç¬¦ï¼‰ï¼Œå…ˆè¿›è¡Œç²¾ç¡®åŒ¹é…
    2. å¯¹äºè¾ƒçŸ­æ–‡æœ¬ï¼ˆå°äºç­‰äº10ä¸ªå­—ç¬¦ï¼‰ï¼Œä½¿ç”¨æ›´ä½çš„ç›¸ä¼¼åº¦é˜ˆå€¼
    
    Args:
        query: è¦æ£€æŸ¥çš„æŸ¥è¯¢æ–‡æœ¬
        
    Returns:
        (is_hit, answer, id): æ˜¯å¦å‘½ä¸­ç¼“å­˜ã€ç¼“å­˜çš„å›ç­”åŠå…¶ID
    """
    if not query or not query.strip():
        return False, None, None
    
    query = query.strip()
    
    # å¯¹äºçŸ­æ–‡æœ¬ï¼Œå…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    if len(query) <= SHORT_TEXT_LENGTH:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT id, answer FROM qa_pairs WHERE question = ?", (query,))
        result = cursor.fetchone()
        conn.close()
        
        if result:
            return True, result[1], result[0]
    
    # æ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œæ‰§è¡Œå‘é‡æœç´¢
    try:
        # é¦–å…ˆè·å–æŸ¥è¯¢çš„å‘é‡åµŒå…¥
        query_embedding = embeddings.embed_query(query)
        
        # æ‰§è¡Œå‘é‡æœç´¢ï¼ˆè·å–æ›´å¤šç»“æœè¿›è¡Œæ‰‹åŠ¨ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        docs_with_scores = vector_db.similarity_search_with_score(
            query=query,
            k=10  # è·å–æ›´å¤šç»“æœï¼Œç„¶åæ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        )
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œè¿”å›æœªå‘½ä¸­
        if not docs_with_scores:
            print("å‘é‡æœç´¢æ— ç»“æœ")
            return False, None, None
        
        # æ‰‹åŠ¨è¿‡æ»¤å‡ºç¼“å­˜çš„é—®é¢˜å¹¶è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cache_results = []
        for doc, faiss_score in docs_with_scores:
            if doc.metadata.get("source") == "cache":
                # è·å–æ–‡æ¡£çš„å‘é‡åµŒå…¥å¹¶è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                doc_embedding = embeddings.embed_query(doc.page_content)
                cosine_sim = cosine_similarity(query_embedding, doc_embedding)
                cache_results.append((doc, cosine_sim))
        
        if not cache_results:
            print("æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜çš„é—®é¢˜")
            return False, None, None
        
        # æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºï¼Œè·å–æœ€ç›¸ä¼¼çš„ç»“æœ
        cache_results.sort(key=lambda x: x[1], reverse=True)
        doc, similarity = cache_results[0]
        
        # å¯¹äºçŸ­æ–‡æœ¬ï¼Œä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
        threshold = SHORT_TEXT_THRESHOLD if len(query) <= 10 else SIMILARITY_THRESHOLD
        
        print(f"ç¼“å­˜æŸ¥è¯¢: {query[:30]}..., æœ€ä½³åŒ¹é…ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}, é˜ˆå€¼: {threshold}")
        
        # ä½™å¼¦ç›¸ä¼¼åº¦è¶Šå¤§è¶Šç›¸ä¼¼
        if similarity >= threshold:
            print(f"ç¼“å­˜å‘½ä¸­: {doc.page_content[:30]}...")
            return True, doc.metadata.get("answer"), doc.metadata.get("id")
        else:
            print(f"ç›¸ä¼¼åº¦ä¸è¶³ï¼Œæœªå‘½ä¸­ç¼“å­˜")
        
    except Exception as e:
        print(f"å‘é‡æœç´¢é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    return False, None, None


def store_qa_pair(question: str, answer: str) -> str:
    """å°†é—®ç­”å¯¹å­˜å‚¨åˆ°SQLiteå’Œå‘é‡æ•°æ®åº“
    
    Args:
        question: è¦å­˜å‚¨çš„é—®é¢˜
        answer: è¦å­˜å‚¨çš„å›ç­”
        
    Returns:
        æ–°åˆ›å»ºçš„é—®ç­”å¯¹ID
    """
    if not question or not answer:
        return ""
    
    question = question.strip()
    answer = answer.strip()
    
    # ç”Ÿæˆå”¯ä¸€ID
    qa_id = str(uuid.uuid4())
    
    try:
        # å­˜å‚¨åˆ°SQLite
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # æ’å…¥é—®ç­”å¯¹
        cursor.execute(
            "INSERT INTO qa_pairs (id, question, answer) VALUES (?, ?, ?)",
            (qa_id, question, answer)
        )
        
        conn.commit()
        conn.close()
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        vector_db.add_texts(
            texts=[question],
            metadatas=[{"answer": answer, "id": qa_id, "source": "cache"}],
            ids=[qa_id]
        )
        
        print(f"å·²ç¼“å­˜é—®ç­”å¯¹: {question[:30]}... -> {answer[:30]}...")
        return qa_id
        
    except Exception as e:
        print(f"å­˜å‚¨é—®ç­”å¯¹æ—¶å‡ºé”™: {str(e)}")
        return ""


#------------------------------------------------------------------------------
# æœç´¢ä»£ç†ä¸RAG
#------------------------------------------------------------------------------

def format_knowledge_results(results):
    """æ ¼å¼åŒ–çŸ¥è¯†åº“æœç´¢ç»“æœä¸ºå¯è¯»æ ¼å¼"""
    if not results:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çŸ¥è¯†åº“å†…å®¹ã€‚"
    
    formatted = "æ‰¾åˆ°çš„ç›¸å…³çŸ¥è¯†ï¼š\n\n"
    for i, item in enumerate(results, 1):
        source = item['metadata'].get('source', 'æœªçŸ¥æ¥æº')
        if source.startswith('knowledge_'):
            source = item['metadata'].get('file_path', 'æœªçŸ¥æ–‡ä»¶')
            if isinstance(source, str) and os.path.exists(source):
                source = os.path.basename(source)
        
        formatted += f"[{i}] æ¥æº: {source}\n"
        formatted += f"å†…å®¹: {item['content'][:200]}{'...' if len(item['content']) > 200 else ''}\n\n"
    
    return formatted


def perform_web_search(query: str) -> str:
    """æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œç›´æ¥è°ƒç”¨æœç´¢API
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        
    Returns:
        æœç´¢ç»“æœæ–‡æœ¬
    """
    try:
        print(f"ğŸ” å¼€å§‹æœç´¢: {query}")
        
        # ç›´æ¥è°ƒç”¨æœç´¢API
        search_results = search_tool.invoke({"query": query})
        
        if not search_results:
            return "æœç´¢æ²¡æœ‰è¿”å›ç»“æœã€‚"
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = []
        for i, result in enumerate(search_results, 1):
            title = result.get('title', 'æ— æ ‡é¢˜')
            content = result.get('content', '')
            url = result.get('url', '')
            
            formatted_result = f"[{i}] {title}\n"
            if content:
                # é™åˆ¶å†…å®¹é•¿åº¦
                content = content[:300] + "..." if len(content) > 300 else content
                formatted_result += f"å†…å®¹: {content}\n"
            if url:
                formatted_result += f"æ¥æº: {url}\n"
            
            formatted_results.append(formatted_result)
        
        result_text = "\n".join(formatted_results)
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        return result_text
        
    except Exception as e:
        error_msg = f"æœç´¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg


#------------------------------------------------------------------------------
# LangGraph å·¥ä½œæµï¼ˆç®€åŒ–ç‰ˆï¼‰
#------------------------------------------------------------------------------

# åˆ›å»ºåº”ç”¨
app = StateGraph(MessagesState)


#------------------------------------------------------------------------------
# FastAPIåº”ç”¨
#------------------------------------------------------------------------------

# APIæ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    web_search: bool = False
    deep_thinking: bool = False
    model: str = "qwen-max"
    conversation_id: Optional[str] = None
    images: Optional[List[Dict[str, Any]]] = None


class ConversationRequest(BaseModel):
    title: Optional[str] = None


class ConversationUpdateRequest(BaseModel):
    title: str


class FileUploadResponse(BaseModel):
    file_id: str
    filename: str
    success: bool
    message: str = ""


# åˆ›å»ºFastAPIåº”ç”¨
fastapi_app = FastAPI(title="æ™ºèƒ½å¯¹è¯ç³»ç»ŸAPI")

# é…ç½®CORSä¸­é—´ä»¶
fastapi_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼Œç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶ä¸ºç‰¹å®šåŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆç”¨äºå‰ç«¯é™æ€èµ„æºï¼‰
from pathlib import Path
templates_dir = Path("templates")
if templates_dir.exists():
    try:
        fastapi_app.mount("/static", StaticFiles(directory="templates"), name="static")
        print("âœ… é™æ€æ–‡ä»¶æœåŠ¡å·²é…ç½®")
    except Exception as e:
        print(f"âš ï¸ é™æ€æ–‡ä»¶æœåŠ¡é…ç½®å¤±è´¥: {e}")

# å…¨å±€åœæ­¢æ ‡å¿—ç®¡ç†
stop_flags = {}
stop_flags_lock = threading.Lock()

def set_stop_flag(websocket_id: str, value: bool):
    """è®¾ç½®åœæ­¢æ ‡å¿—"""
    with stop_flags_lock:
        stop_flags[websocket_id] = value

def get_stop_flag(websocket_id: str) -> bool:
    """è·å–åœæ­¢æ ‡å¿—"""
    with stop_flags_lock:
        return stop_flags.get(websocket_id, False)

def clear_stop_flag(websocket_id: str):
    """æ¸…é™¤åœæ­¢æ ‡å¿—"""
    with stop_flags_lock:
        stop_flags.pop(websocket_id, None)

# å­˜å‚¨WebSocketè¿æ¥
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.websocket_ids = {}  # å­˜å‚¨websocketåˆ°IDçš„æ˜ å°„

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        websocket_id = str(uuid.uuid4())
        self.active_connections.append(websocket)
        self.websocket_ids[websocket] = websocket_id
        set_stop_flag(websocket_id, False)
        return websocket_id

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        websocket_id = self.websocket_ids.pop(websocket, None)
        if websocket_id:
            clear_stop_flag(websocket_id)

    def get_websocket_id(self, websocket: WebSocket) -> str:
        return self.websocket_ids.get(websocket, "")

    async def send_message(self, message: str, websocket: WebSocket):
        try:
            await websocket.send_text(message)
        except Exception as e:
            print(f"å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}")
            self.disconnect(websocket)


manager = ConnectionManager()


@fastapi_app.get("/")
async def read_root():
    """è¿”å›å‰ç«¯é¡µé¢"""
    from fastapi.responses import FileResponse
    from pathlib import Path
    
    # æ£€æŸ¥å‰ç«¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    index_file = Path("templates") / "index.html"
    if index_file.exists():
        return FileResponse(str(index_file))
    else:
        return {
            "message": "æ™ºèƒ½å¯¹è¯ç³»ç»ŸAPI",
            "error": "å‰ç«¯æ–‡ä»¶ templates/index.html ä¸å­˜åœ¨",
            "api_docs": "/docs",
            "suggestion": "è¯·ç¡®ä¿ templates/index.html æ–‡ä»¶å­˜åœ¨"
        }


@fastapi_app.post("/chat")
async def chat(request: ChatRequest):
    """å¤„ç†èŠå¤©è¯·æ±‚å¹¶è¿”å›å›ç­”"""
    if not request.message.strip():
        return JSONResponse(content={"é”™è¯¯": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}, status_code=400)
    
    # å¦‚æœæœ‰å¯¹è¯ IDï¼ŒéªŒè¯å®ƒçš„å­˜åœ¨æ€§
    conversation_id = request.conversation_id
    if conversation_id:
        conversation = get_conversation(conversation_id)
        if not conversation:
            return JSONResponse(content={"é”™è¯¯": "å¯¹è¯ä¸å­˜åœ¨"}, status_code=404)
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¹è¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°å¯¹è¯
        new_conversation = create_conversation()
        conversation_id = new_conversation["id"]
    
    # æ„å»ºè¾“å…¥çŠ¶æ€
    state = {
        "messages": [HumanMessage(content=request.message)],
        "web_search_requested": request.web_search
    }
    
    # è°ƒç”¨å·¥ä½œæµåº”ç”¨
    result = app.invoke(state)
    
    # æå–å›ç­”
    ai_message = result["messages"][-1]
    
    # å°†é—®ç­”å¯¹æ·»åŠ åˆ°å¯¹è¯ä¸­
    add_message_to_conversation(conversation_id, request.message, ai_message.content)
    
    return {
        "response": ai_message.content,
        "conversation_id": conversation_id
    }


async def generate_chat_response(message: str, web_search: bool, deep_thinking: bool = False, model_id: str = "qwen-max", images: Optional[List[Dict[str, Any]]] = None, conversation_id: Optional[str] = None, websocket_id: str = None) -> AsyncGenerator[str, None]:
    """ç”ŸæˆèŠå¤©å›ç­”çš„å¼‚æ­¥ç”Ÿæˆå™¨"""
    full_response = ""
    thinking_process = ""
    
    # å¤„ç†å›¾ç‰‡æ•°æ®
    has_images = images and len(images) > 0
    processed_images = []
    if has_images:
        processed_images = process_images(images)
        print(f"å¤„ç†äº† {len(processed_images)} å¼ å›¾ç‰‡")
    
    try:
        # 1. é¦–å…ˆæ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæ²¡æœ‰å›¾ç‰‡å’Œç½‘ç»œæœç´¢ï¼‰
        if not has_images and not web_search:
            is_hit, cached_answer, cache_id = check_cache(message)
            if is_hit:
                print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè¿”å›ç¼“å­˜ç»“æœ: {cache_id}")
                yield cached_answer
                full_response = cached_answer
                return
        
        # 2. åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†ä¸­ï¼ˆå¦‚æœæœ‰å¯¹è¯IDï¼‰
        if conversation_id:
            load_conversation_history(conversation_id)
        
        # 3. è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å†å²è®°å¿†ï¼‰
        context_messages = get_conversation_context(conversation_id, message)
        
        # 4. å¦‚æœå¼€å¯äº†è”ç½‘æœç´¢ï¼Œç›´æ¥è°ƒç”¨æœç´¢API
        search_results = ""
        if web_search:
            search_results = perform_web_search(message)
            # å°†æœç´¢ç»“æœæ·»åŠ åˆ°å½“å‰æ¶ˆæ¯ä¸­
            search_context = f"\n\nè”ç½‘æœç´¢ç»“æœï¼š\n{search_results}\n\nè¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            message_with_search = message + search_context
            # æ›´æ–°ä¸Šä¸‹æ–‡æ¶ˆæ¯çš„æœ€åä¸€æ¡
            context_messages[-1] = HumanMessage(content=message_with_search)
        else:
            message_with_search = message
        
        # 5. æŸ¥è¯¢çŸ¥è¯†åº“
        knowledge_results = query_knowledge_base(message_with_search)
        if knowledge_results:
            knowledge_context = f"\n\nçŸ¥è¯†åº“å†…å®¹ï¼š\n{format_knowledge_results(knowledge_results)}\n\n"
            message_with_search += knowledge_context
            # æ›´æ–°ä¸Šä¸‹æ–‡æ¶ˆæ¯çš„æœ€åä¸€æ¡
            context_messages[-1] = HumanMessage(content=message_with_search)
        
        # 6. å¤„ç†å›¾ç‰‡ç†è§£ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if has_images and not web_search:
            print("ğŸ–¼ï¸ ä½¿ç”¨è§†è§‰æ¨¡å‹å¤„ç†å›¾ç‰‡")
            
            try:
                # åˆ›å»ºè§†è§‰æ¨¡å‹
                vision_model = get_model('qwen-vl-max', False)
                
                # æ„å»ºåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯ - ä½¿ç”¨æ­£ç¡®çš„OpenAIæ ¼å¼
                message_content = []
                
                # æ·»åŠ æ–‡æœ¬é—®é¢˜
                if message.strip():
                    message_content.append({
                        "type": "text", 
                        "text": message
                    })
                else:
                    message_content.append({
                        "type": "text", 
                        "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"
                    })
                
                # æ·»åŠ å›¾ç‰‡ï¼Œä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
                for img in processed_images:
                    image_url = img.get('url', '')
                    if image_url:
                        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„image_urlæ ¼å¼
                        message_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": image_url
                            }
                        })
                
                # åˆ›å»ºæ¶ˆæ¯
                vision_message = HumanMessage(content=message_content)
                
                # æµå¼ç”Ÿæˆå›ç­”
                async for chunk in vision_model.astream([vision_message]):
                    if websocket_id and get_stop_flag(websocket_id):
                        yield "\n\n[å¯¹è¯å·²åœæ­¢]"
                        return
                    
                    chunk_content = chunk.content
                    full_response += chunk_content
                    yield chunk_content
                
            except Exception as e:
                error_msg = f"å›¾ç‰‡ç†è§£å¤±è´¥: {str(e)}"
                print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_msg}")
                
                # å¦‚æœæ˜¯æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨æ‚¨ç¤ºä¾‹ä¸­çš„æ ¼å¼
                try:
                    print("å°è¯•ä½¿ç”¨å¤‡ç”¨æ ¼å¼å¤„ç†å›¾ç‰‡")
                    # ä½¿ç”¨æ‚¨çš„image_test.pyä¸­çš„æ ¼å¼
                    from image_test import QwenVLChatModel
                    
                    # åˆ›å»ºQwenVLæ¨¡å‹å®ä¾‹
                    qwen_vl = QwenVLChatModel(streaming=True)
                    
                    # æå–å›¾ç‰‡URL
                    image_urls = []
                    for img in processed_images:
                        image_urls.append(img.get('url', ''))
                    
                    # åˆ›å»ºåŒ…å«å›¾åƒçš„æ¶ˆæ¯ï¼ˆä½¿ç”¨æ‚¨çš„ç¤ºä¾‹æ ¼å¼ï¼‰
                    vision_message = HumanMessage(
                        content=message if message.strip() else "è¯·åˆ†æè¿™å¼ å›¾ç‰‡",
                        additional_kwargs={"images": image_urls}
                    )
                    
                    # ç”Ÿæˆå“åº”
                    result = qwen_vl._generate([vision_message])
                    full_response = result.generations[0].message.content
                    yield full_response
                    
                except Exception as backup_error:
                    error_msg = f"å›¾ç‰‡ç†è§£å®Œå…¨å¤±è´¥: ä¸»è¦é”™è¯¯: {str(e)}, å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(backup_error)}"
                    print(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {str(backup_error)}")
                    yield error_msg
                    return
        
        else:
            # 7. ä½¿ç”¨æ™®é€šæ¨¡å‹å¤„ç†æ–‡æœ¬ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡è®°å¿†ï¼‰
            current_model = get_model(model_id, deep_thinking)
            
            # å¯¹äºæ”¯æŒæ€è€ƒçš„æ¨¡å‹ï¼Œä½¿ç”¨æ€è€ƒå·¥å…·
            if deep_thinking and model_id in ['qwen3-235b-a22b', 'qwen-plus-latest', 'deepseek-r1']:
                thinking_started = False
                content_started = False
                
                for chunk_data in thinking_tool.think_and_respond(
                    question=message_with_search,
                    model=model_id,
                    enable_thinking=True
                ):
                    if websocket_id and get_stop_flag(websocket_id):
                        yield "\n\n[å¯¹è¯å·²åœæ­¢]"
                        return
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ - ç«‹å³æµå¼è¾“å‡º
                    if chunk_data["type"] == "thinking" and chunk_data["thinking"]:
                        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¾“å‡ºæ€è€ƒå†…å®¹ï¼Œæ·»åŠ æ ‡é¢˜
                        if not thinking_started:
                            yield "**ğŸ¤” æ€è€ƒè¿‡ç¨‹ï¼š**\n"
                            thinking_started = True
                        
                        # ç«‹å³æµå¼è¾“å‡ºæ€è€ƒå†…å®¹ï¼Œä¸ç´¯ç§¯
                        yield chunk_data["thinking"]
                    
                    # å¤„ç†å›ç­”å†…å®¹
                    if chunk_data["type"] == "content" and chunk_data["content"]:
                        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¾“å‡ºå›ç­”å†…å®¹ï¼Œæ·»åŠ åˆ†éš”å’Œæ ‡é¢˜
                        if not content_started:
                            if thinking_started:
                                yield "\n\n**ğŸ’¡ å›ç­”ï¼š**\n"
                            content_started = True
                        
                        full_response += chunk_data["content"]
                        yield chunk_data["content"]
                    
                    if chunk_data["type"] == "error":
                        yield chunk_data["content"]
                        return
            else:
                # æ™®é€šæµå¼å“åº”ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡è®°å¿†ï¼‰
                async for chunk in current_model.astream(context_messages):
                    if websocket_id and get_stop_flag(websocket_id):
                        yield "\n\n[å¯¹è¯å·²åœæ­¢]"
                        return
                    
                    chunk_content = chunk.content
                    full_response += chunk_content
                    yield chunk_content
        
        # 8. ä¿å­˜å¯¹è¯åˆ°è®°å¿†ä¸­
        if conversation_id and message and full_response:
            save_to_memory(conversation_id, message, full_response)
    
    except Exception as e:
        error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        yield error_msg
        return
    
    # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œä½¿ç”¨finallyå—ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨
    # QAä¿å­˜é€»è¾‘å°†åœ¨WebSocketå¤„ç†å‡½æ•°ä¸­å¤„ç†


@fastapi_app.post("/chat/stream")
async def stream_chat(request: Request):
    """æµå¼å¤„ç†èŠå¤©è¯·æ±‚å¹¶è¿”å›å›ç­”"""
    data = await request.json()
    message = data.get("message", "")
    web_search = data.get("web_search", False)
    conversation_id = data.get("conversation_id", None)
    
    if not message.strip():
        return JSONResponse(content={"error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}, status_code=400)
    
    # éªŒè¯å¯¹è¯å­˜åœ¨æ€§æˆ–åˆ›å»ºæ–°å¯¹è¯
    if conversation_id:
        conversation = get_conversation(conversation_id)
        if not conversation:
            return JSONResponse(content={"error": "å¯¹è¯ä¸å­˜åœ¨"}, status_code=404)
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¹è¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°å¯¹è¯
        new_conversation = create_conversation()
        conversation_id = new_conversation["id"]
        
    # å¼€å§‹æµå¼ç”Ÿæˆå›ç­”
    return StreamingResponse(
        generate_chat_response(message, web_search, conversation_id=conversation_id),
        media_type="text/plain"
    )


@fastapi_app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    websocket_id = await manager.connect(websocket)
    
    try:
        while True:
            # æ¥æ”¶æ¶ˆæ¯
            data = await websocket.receive_text()
            parsed_data = json.loads(data)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åœæ­¢è¯·æ±‚
            if parsed_data.get("action") == "stop":
                # è®¾ç½®åœæ­¢æ ‡å¿—
                set_stop_flag(websocket_id, True)
                # å‘é€åœæ­¢ç¡®è®¤
                await manager.send_message(json.dumps({"stopped": True}), websocket)
                continue
            
            message = parsed_data.get("message", "")
            web_search = parsed_data.get("web_search", False)
            deep_thinking = parsed_data.get("deep_thinking", False)
            model_id = parsed_data.get("model", "qwen-max")
            images = parsed_data.get("images", None)
            conversation_id = parsed_data.get("conversation_id")
            
            # ç¡®ä¿messageæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if isinstance(message, list):
                # å¦‚æœmessageæ˜¯åˆ—è¡¨ï¼Œæå–æ–‡æœ¬å†…å®¹
                text_parts = []
                for item in message:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif isinstance(item, str):
                        text_parts.append(item)
                message = " ".join(text_parts).strip()
            elif not isinstance(message, str):
                message = str(message)
            
            # å¦‚æœmessageä¸ºç©ºä½†æœ‰å›¾ç‰‡ï¼Œè®¾ç½®é»˜è®¤æ¶ˆæ¯
            if not message.strip() and images:
                message = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"
            
            if not message.strip() and not images:
                await manager.send_message(json.dumps({"error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}), websocket)
                continue
            
            # é‡ç½®åœæ­¢æ ‡å¿—
            set_stop_flag(websocket_id, False)
            
            # å¦‚æœæ²¡æœ‰å¯¹è¯ IDï¼Œåˆ›å»ºä¸€ä¸ªæ–°å¯¹è¯
            if not conversation_id:
                # åˆ›å»ºæ–°å¯¹è¯
                new_conversation = create_conversation()
                conversation_id = new_conversation["id"]
                # å°†æ–°å¯¹è¯çš„ä¿¡æ¯å‘é€ç»™å®¢æˆ·ç«¯
                await manager.send_message(json.dumps({"new_conversation": new_conversation}), websocket)
            
            # å¦‚æœå¯ç”¨äº†ç½‘ç»œæœç´¢ï¼Œç«‹å³å‘é€æ­£åœ¨æœç´¢çš„é€šçŸ¥
            if web_search:
                await manager.send_message(json.dumps({"chunk": "æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...è¯·ç¨å€™\n\n"}), websocket)
            
            # æµå¼ç”Ÿæˆå›ç­”ï¼ˆæœç´¢å°†åœ¨generate_chat_responseä¸­å¤„ç†ï¼‰
            full_response = ""
            try:
                async for chunk in generate_chat_response(message, web_search, deep_thinking, model_id, images, conversation_id, websocket_id):
                    # æ£€æŸ¥è¿æ¥æ˜¯å¦è¿˜æ´»è·ƒ
                    if websocket not in manager.active_connections:
                        break
                    await manager.send_message(json.dumps({"chunk": chunk}), websocket)
                    full_response += chunk
                
                # å‘é€å®Œæˆä¿¡å·ï¼ˆåªæœ‰åœ¨æ²¡æœ‰åœæ­¢çš„æƒ…å†µä¸‹ï¼‰
                if not get_stop_flag(websocket_id):
                    # ä¿å­˜QAå¯¹è¯åˆ°ç¼“å­˜å’Œå¯¹è¯è®°å½•
                    try:
                        # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼‰
                        if not images and full_response and message:
                            store_qa_pair(message, full_response)
                            print(f"âœ… å·²ä¿å­˜QAå¯¹è¯åˆ°ç¼“å­˜: {message[:30]}...")
                        
                        # ä¿å­˜åˆ°å¯¹è¯è®°å½•
                        if conversation_id and message and full_response:
                            add_message_to_conversation(conversation_id, message, full_response)
                            print(f"âœ… å·²ä¿å­˜QAå¯¹è¯åˆ°å¯¹è¯è®°å½•: å¯¹è¯ID {conversation_id}")
                            
                    except Exception as save_error:
                        print(f"âŒ ä¿å­˜QAå¯¹è¯æ—¶å‡ºé”™: {str(save_error)}")
                    
                    await manager.send_message(json.dumps({
                        "complete": True, 
                        "full_response": full_response,
                        "conversation_id": conversation_id
                    }), websocket)
                else:
                    # å¦‚æœæ˜¯åœæ­¢çš„ï¼Œå‘é€åœæ­¢å®Œæˆä¿¡å·
                    await manager.send_message(json.dumps({
                        "stopped_complete": True,
                        "partial_response": full_response,
                        "conversation_id": conversation_id
                    }), websocket)
            except Exception as e:
                # å¤„ç†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”™è¯¯
                error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
                await manager.send_message(json.dumps({"error": error_msg}), websocket)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        print(f"WebSocketå¤„ç†å‡ºé”™: {str(e)}")
        try:
            await manager.send_message(json.dumps({"error": f"è¿æ¥å‡ºé”™: {str(e)}"}), websocket)
        except:
            pass
        manager.disconnect(websocket)


@fastapi_app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    original_filename = file.filename
    file_extension = original_filename.split(".")[-1].lower()
    
    supported_extensions = ["pdf", "csv", "txt", "md", "xls", "xlsx", "docx", "doc", "json"]
    
    if file_extension not in supported_extensions:
        return FileUploadResponse(
            file_id="",
            filename=original_filename,
            success=False,
            message=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„æ ¼å¼ï¼š{', '.join(supported_extensions)}"
        )
    
    # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{str(uuid.uuid4())}.{file_extension}"
    file_path = os.path.join(KNOWLEDGE_DIR, unique_filename)
    
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, "wb") as buffer:
        buffer.write(await file.read())
    
    # å¤„ç†æ–‡ä»¶å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“
    result = add_file_to_knowledge_base(file_path, original_filename, file_extension)
    
    return FileUploadResponse(
        file_id=result["file_id"],
        filename=original_filename,
        success=result["success"],
        message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸå¹¶å·²æ·»åŠ åˆ°çŸ¥è¯†åº“" if result["success"] else "æ–‡ä»¶ä¸Šä¼ æˆåŠŸä½†å¤„ç†å¤±è´¥"
    )


@fastapi_app.get("/knowledge/files")
async def list_knowledge_files():
    """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    return get_knowledge_base_files()


@fastapi_app.delete("/knowledge/files/{file_id}")
async def delete_knowledge_file(file_id: str):
    """ä»çŸ¥è¯†åº“ä¸­åˆ é™¤æ–‡ä»¶"""
    try:
        # é¦–å…ˆè·å–æ–‡ä»¶ä¿¡æ¯
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT file_path FROM knowledge_files WHERE id = ?", (file_id,))
        result = cursor.fetchone()
        
        if not result:
            conn.close()
            return {"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        file_path = result[0]
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤æ–‡ä»¶è®°å½•
        cursor.execute("DELETE FROM knowledge_files WHERE id = ?", (file_id,))
        conn.commit()
        
        # å°è¯•åˆ é™¤ç‰©ç†æ–‡ä»¶
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"å·²åˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
        except Exception as file_error:
            print(f"åˆ é™¤ç‰©ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(file_error)}")
        
        # é‡å»ºå‘é‡æ•°æ®åº“ - åˆ é™¤æ–‡ä»¶å¯¹åº”çš„æ–‡æ¡£
        global vector_db
        try:
            # è·å–å½“å‰æ‰€æœ‰æ–‡æ¡£
            docs = vector_db.similarity_search_with_score("", k=1000)
            
            # åˆ›å»ºæ–°çš„FAISSç´¢å¼•
            vector_db = init_faiss_index()
            
            # é‡æ–°æ·»åŠ éåˆ é™¤æ–‡ä»¶çš„æ–‡æ¡£
            for doc, _ in docs:
                doc_source = doc.metadata.get("source", "")
                if not doc_source.startswith(f"knowledge_{file_id}"):
                    vector_db.add_texts([doc.page_content], [doc.metadata])
            
            print(f"å·²ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤æ–‡ä»¶ {file_id} å¯¹åº”çš„æ–‡æ¡£")
        except Exception as vector_error:
            print(f"æ›´æ–°å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(vector_error)}")
        
        conn.close()
        
        return {"success": True, "message": "å·²æˆåŠŸåˆ é™¤æ–‡ä»¶"}
    except Exception as e:
        print(f"åˆ é™¤çŸ¥è¯†åº“æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return {"success": False, "message": f"åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"}


@fastapi_app.delete("/conversations/{conversation_id}")
async def delete_conversation_endpoint(conversation_id: str):
    """åˆ é™¤æŒ‡å®šçš„å¯¹è¯"""
    try:
        print(f"å°è¯•åˆ é™¤å¯¹è¯: {conversation_id}")
        # éªŒè¯å¯¹è¯å­˜åœ¨æ€§
        conversation = get_conversation(conversation_id)
        if not conversation:
            return {"success": False, "message": "å¯¹è¯ä¸å­˜åœ¨"}
        
        # æ‰§è¡Œåˆ é™¤
        success = delete_conversation(conversation_id)
        if success:
            return {"success": True, "message": "å¯¹è¯åˆ é™¤æˆåŠŸ"}
        else:
            return {"success": False, "message": "åˆ é™¤å¯¹è¯æ—¶å‡ºé”™"}
    except Exception as e:
        print(f"åˆ é™¤å¯¹è¯æ—¶å‡ºé”™: {str(e)}")
        return {"success": False, "message": f"åˆ é™¤å¯¹è¯æ—¶å‡ºé”™: {str(e)}"}


@fastapi_app.put("/conversations/{conversation_id}")
async def rename_conversation_endpoint(conversation_id: str, request: Request):
    """é‡å‘½åå¯¹è¯"""
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = await request.json()
        new_title = data.get("title")
        
        if not new_title or not new_title.strip():
            return {"success": False, "message": "æ–°æ ‡é¢˜ä¸èƒ½ä¸ºç©º"}
        
        # éªŒè¯å¯¹è¯å­˜åœ¨æ€§
        conversation = get_conversation(conversation_id)
        if not conversation:
            return {"success": False, "message": "å¯¹è¯ä¸å­˜åœ¨"}
        
        # æ‰§è¡Œé‡å‘½å
        success = update_conversation_title(conversation_id, new_title)
        if success:
            return {"success": True, "message": "å¯¹è¯é‡å‘½åæˆåŠŸ", "title": new_title}
        else:
            return {"success": False, "message": "é‡å‘½åå¯¹è¯æ—¶å‡ºé”™"}
    except Exception as e:
        print(f"é‡å‘½åå¯¹è¯æ—¶å‡ºé”™: {str(e)}")
        return {"success": False, "message": f"é‡å‘½åå¯¹è¯æ—¶å‡ºé”™: {str(e)}"}


@fastapi_app.get("/supported-formats")
async def get_supported_formats():
    """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
    return {
        "formats": [
            {"extension": "pdf", "description": "PDFæ–‡æ¡£"},
            {"extension": "csv", "description": "CSVè¡¨æ ¼æ–‡ä»¶"},
            {"extension": "txt", "description": "æ–‡æœ¬æ–‡ä»¶"},
            {"extension": "md", "description": "Markdownæ–‡æ¡£"},
            {"extension": "xls", "description": "Excelå·¥ä½œç°¿ (æ—§ç‰ˆ)"},
            {"extension": "xlsx", "description": "Excelå·¥ä½œç°¿"},
            {"extension": "docx", "description": "Wordæ–‡æ¡£"},
            {"extension": "doc", "description": "Wordæ–‡æ¡£ (æ—§ç‰ˆ)"},
            {"extension": "json", "description": "JSONæ•°æ®æ–‡ä»¶"}
        ]
    }


# å¯¹è¯ç®¡ç†API
@fastapi_app.post("/conversations")
async def create_new_conversation(request: ConversationRequest = None):
    """åˆ›å»ºæ–°å¯¹è¯"""
    title = None
    if request and request.title:
        title = request.title
    
    conversation = create_conversation(title)
    return conversation


@fastapi_app.get("/conversations")
async def list_conversations():
    """è·å–æ‰€æœ‰å¯¹è¯åˆ—è¡¨"""
    return get_all_conversations()


@fastapi_app.get("/conversations/{conversation_id}")
async def get_conversation_detail(conversation_id: str):
    """è·å–å•ä¸ªå¯¹è¯è¯¦æƒ…"""
    conversation = get_conversation(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")
    
    return conversation


@fastapi_app.put("/conversations/{conversation_id}")
async def update_conversation(conversation_id: str, request: ConversationUpdateRequest):
    """æ›´æ–°å¯¹è¯ä¿¡æ¯"""
    conversation = get_conversation(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")
    
    success = update_conversation_title(conversation_id, request.title)
    if not success:
        raise HTTPException(status_code=500, detail="æ›´æ–°å¯¹è¯å¤±è´¥")
    
    return {"id": conversation_id, "title": request.title, "updated": True}


@fastapi_app.delete("/conversations/{conversation_id}")
async def delete_conversation_endpoint(conversation_id: str):
    """åˆ é™¤å¯¹è¯"""
    conversation = get_conversation(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")
    
    success = delete_conversation(conversation_id)
    if not success:
        raise HTTPException(status_code=500, detail="åˆ é™¤å¯¹è¯å¤±è´¥")
    
    return {"id": conversation_id, "deleted": True}


@fastapi_app.get("/conversations/{conversation_id}/messages")
async def get_conversation_messages_endpoint(conversation_id: str):
    """è·å–å¯¹è¯ä¸­çš„æ‰€æœ‰æ¶ˆæ¯"""
    conversation = get_conversation(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")
    
    messages = get_conversation_messages(conversation_id)
    return messages


@fastapi_app.get("/random-name")
async def get_random_name():
    """è·å–éšæœºå¯çˆ±çš„åå­—"""
    try:
        return {"name": generate_cute_name()}
    except Exception as e:
        print(f"ç”Ÿæˆéšæœºåå­—æ—¶å‡ºé”™: {str(e)}")
        return {"name": f"å¯¹è¯ {datetime.now().strftime('%m-%d %H:%M')}"}


@fastapi_app.get("/models")
async def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "models": [
            {
                "id": "qwen-max",
                "name": "Qwen-Max",
                "description": "æœ€å¼ºå¤§çš„é€šç”¨æ¨¡å‹",
                "supports_vision": False,
                "supports_thinking": False
            },
            {
                "id": "qwen-plus", 
                "name": "Qwen-Plus",
                "description": "å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡çš„æ¨¡å‹",
                "supports_vision": False,
                "supports_thinking": False
            },
            {
                "id": "qwen-plus-latest",
                "name": "Qwen-Plus-Latest",
                "description": "æœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒæ€è€ƒå’Œéæ€è€ƒåˆ‡æ¢",
                "supports_vision": False,
                "supports_thinking": True
            },
            {
                "id": "qwen3-235b-a22b",
                "name": "Qwen3-235B",
                "description": "æœ€æ–°çš„Qwen3æ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒ",
                "supports_vision": False,
                "supports_thinking": True
            },
            {
                "id": "qwen-vl-max",
                "name": "Qwen-VL-Max",
                "description": "è§†è§‰ç†è§£æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡åˆ†æ",
                "supports_vision": True,
                "supports_thinking": False
            },
            {
                "id": "deepseek-r1",
                "name": "DeepSeek-R1",
                "description": "æ·±åº¦æ€è€ƒæ¨¡å‹ï¼Œè‡ªåŠ¨å¼€å¯æ€è€ƒè¿‡ç¨‹",
                "supports_vision": False,
                "supports_thinking": True,
                "force_thinking": True
            }
        ]
    }


@fastapi_app.post("/screenshot")
async def trigger_screenshot():
    """è§¦å‘æˆªå›¾åŠŸèƒ½"""
    try:
        # è¿™é‡Œå¯ä»¥é›†æˆæˆªå›¾å·¥å…·ï¼Œæ¯”å¦‚ä½¿ç”¨pyautogui
        # ç”±äºå®‰å…¨è€ƒè™‘ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªæç¤ºä¿¡æ¯
        return {
            "success": True,
            "message": "è¯·ä½¿ç”¨ç³»ç»Ÿæˆªå›¾å·¥å…·ï¼ˆå¦‚Windowsçš„Win+Shift+Sï¼‰è¿›è¡Œæˆªå›¾ï¼Œç„¶åç²˜è´´åˆ°èŠå¤©æ¡†ä¸­"
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"æˆªå›¾åŠŸèƒ½å‡ºé”™: {str(e)}"
        }


#------------------------------------------------------------------------------
# äº¤äº’å¼èŠå¤©ï¼ˆå‘½ä»¤è¡Œï¼‰
#------------------------------------------------------------------------------

def interactive_chat():
    """äº¤äº’å¼èŠå¤©åŠŸèƒ½
    
    å…è®¸ç”¨æˆ·ä¸æ™ºèƒ½åŠ©æ‰‹è¿›è¡Œå¯¹è¯ï¼Œæ”¯æŒç¼“å­˜å’Œæœç´¢åŠŸèƒ½
    """
    print("\næ¬¢è¿ä½¿ç”¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ!")
    print("è¾“å…¥'exit'æˆ–'quit'é€€å‡ºå¯¹è¯\n")
    
    history = {"messages": []}
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ")
        
        # æ£€æŸ¥æ˜¯å¦é€€å‡º
        if user_input.lower() in ["exit", "quit"]:
            print("\nè°¢è°¢ä½¿ç”¨ï¼Œå†è§!")
            break
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æœç´¢
        web_search = False
        if user_input.startswith("/search "):
            user_input = user_input[8:].strip()  # ç§»é™¤æŒ‡ä»¤å‰ç¼€
            web_search = True
            print("(å·²å¯ç”¨è”ç½‘æœç´¢)")
        
        # æ›´æ–°çŠ¶æ€
        state = {
            "messages": history["messages"] + [HumanMessage(content=user_input)],
            "web_search_requested": web_search
        }
        
        # è°ƒç”¨åº”ç”¨
        result = app.invoke(state)
        
        # æå–å›ç­”å¹¶æ‰“å°
        ai_message = result["messages"][-1]
        print(f"\nåŠ©æ‰‹: {ai_message.content}")
        
        # æ›´æ–°å†å²
        history["messages"] = result["messages"]


#------------------------------------------------------------------------------
# ä¸»ç¨‹åºå…¥å£
#------------------------------------------------------------------------------

def main():
    """å¯åŠ¨åº”ç”¨çš„ä¸»å…¥å£ç‚¹"""
    import uvicorn
    uvicorn.run(fastapi_app, host="0.0.0.0", port=8000)


if __name__ == "__main__":
    print("æ™ºèƒ½å¯¹è¯ç³»ç»Ÿæ­£åœ¨å¯åŠ¨...")
    main()
